{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install nltk\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModel\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from datasets import *\n",
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers import AutoConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import Trainer\n",
    "import nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f85c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testSet = pd.read_csv('0_Tag.csv')\n",
    "\n",
    "bert_model_name = \"xlm-roberta-base\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03979b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#bert_model_name = \"bert-base-uncased\"\n",
    "bert_model_name = \"xlm-roberta-base\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "\n",
    "\n",
    "def align_labels(examples):\n",
    "    tokenized_inputs = bert_tokenizer(examples[\"Text\"], truncation=True,\n",
    "                                      is_split_into_words=True)\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"Label\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"Label\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d9322b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokensSet=[]\n",
    "for row in testSet['Text']:\n",
    "    tokensSet.append(bert_tokenizer(row).tokens())\n",
    "\n",
    "testSet['tokens']=tokensSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371e35be",
   "metadata": {},
   "source": [
    "Read and tokenize some files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21acc0c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_body' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39msent_tokenize(\u001b[43mtext_body\u001b[49m, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_body' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.tokenize.sent_tokenize(text_body, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc89d87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "#theFiles=[\"0_Tag.csv\",\"1_Tag.csv\",\"2_Tag.csv\",\"3_Tag.csv\",\"4_Tag.csv\",\"5_Tag.csv\",\"6_Tag.csv\"]\n",
    "\n",
    "for i in range(0,14):\n",
    "    print(i)\n",
    "    testSet = pd.read_csv(str(i) + '_Tag.csv')\n",
    "    bertSet=[]\n",
    "    bert_model_name = \"xlm-roberta-base\"\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "    tokensSet=[]\n",
    "    for row in testSet['Text']:\n",
    "        #print(row)\n",
    "        tokensSet.append(bert_tokenizer(row).tokens())\n",
    "        bertSet.append(bert_tokenizer(row))\n",
    "\n",
    "    testSet['tokens']=tokensSet\n",
    "    testSet['berts']=bertSet\n",
    "    testSet.to_csv(str(i) + '_Tag_Tokenize.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a0ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_ckpt = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "#print(tokenizer.model_max_length)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['Text'], padding=True, truncation=True, max_length=tokenizer.model_max_length)\n",
    "\n",
    "\n",
    "#theDS_encoded = testSet.map(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458b992",
   "metadata": {},
   "source": [
    "Read in the files to a dataset for easier interpretation.  These have been manually tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "73499985",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafiles=[]\n",
    "for i in range(0,14):\n",
    "    datafiles.append(str(i) + '_Tag.csv')\n",
    "\n",
    "alldata=pd.DataFrame()\n",
    "for i in datafiles:\n",
    "    allData=pd.concat([alldata,pd.read_csv(i).T])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d569058d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'Text', 'Label'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class_names = ['O', 'PRESENT','ABSENT','VOTE','PROJ']\n",
    "\n",
    "ft = Features({'Text': Value('string'), 'Label': ClassLabel(num_classes=5,names=class_names)})\n",
    "testDataset = load_dataset(\"csv\", 'testing_PD.csv',)\n",
    "testDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "64293d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=pd.read_csv('0_Tag_Trim.csv')\n",
    "#str(x) for x in xs\n",
    "exText = [' '.join(text1['Text'].to_list())]\n",
    "exLabel = [','.join(str(x) for x in text1['Label'].to_list())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc77d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "testingPD=pd.DataFrame()\n",
    "testingPD['Text'] = exText\n",
    "testingPD['Label'] = exLabel\n",
    "\n",
    "#testingPD.to_csv('testing_PD.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907aa9fb",
   "metadata": {},
   "source": [
    "Starting another group to work through class 12 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "\n",
    "\n",
    "bert_model_name = \"bert-base-cased\"\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "4e42f542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5691 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5956 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "bert_tokens = bert_tokenizer(exText[0]).tokens()\n",
    "xlmr_tokens = xlmr_tokenizer(exText[0]).tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a3df5716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5946</th>\n",
       "      <th>5947</th>\n",
       "      <th>5948</th>\n",
       "      <th>5949</th>\n",
       "      <th>5950</th>\n",
       "      <th>5951</th>\n",
       "      <th>5952</th>\n",
       "      <th>5953</th>\n",
       "      <th>5954</th>\n",
       "      <th>5955</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BERT</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>Hall</td>\n",
       "      <td>,</td>\n",
       "      <td>with</td>\n",
       "      <td>Co</td>\n",
       "      <td>-</td>\n",
       "      <td>Chair</td>\n",
       "      <td>##person</td>\n",
       "      <td>,</td>\n",
       "      <td>Jean</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLM-R</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Hall</td>\n",
       "      <td>,</td>\n",
       "      <td>▁with</td>\n",
       "      <td>▁Co</td>\n",
       "      <td>▁-</td>\n",
       "      <td>Cha</td>\n",
       "      <td>ir</td>\n",
       "      <td>person</td>\n",
       "      <td>,</td>\n",
       "      <td>...</td>\n",
       "      <td>▁Plan</td>\n",
       "      <td>ning</td>\n",
       "      <td>▁Board</td>\n",
       "      <td>▁Secretary</td>\n",
       "      <td>▁Plan</td>\n",
       "      <td>ning</td>\n",
       "      <td>▁Technic</td>\n",
       "      <td>ia</td>\n",
       "      <td>▁n</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 5956 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1    2      3    4    5      6         7       8     9     ...  \\\n",
       "BERT   [CLS]   Hall    ,   with   Co    -  Chair  ##person       ,  Jean  ...   \n",
       "XLM-R    <s>  ▁Hall    ,  ▁with  ▁Co   ▁-    Cha        ir  person     ,  ...   \n",
       "\n",
       "        5946  5947    5948        5949   5950  5951      5952  5953  5954  \\\n",
       "BERT    None  None    None        None   None  None      None  None  None   \n",
       "XLM-R  ▁Plan  ning  ▁Board  ▁Secretary  ▁Plan  ning  ▁Technic    ia    ▁n   \n",
       "\n",
       "       5955  \n",
       "BERT   None  \n",
       "XLM-R  </s>  \n",
       "\n",
       "[2 rows x 5956 columns]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([bert_tokens, xlmr_tokens], index=[\"BERT\", \"XLM-R\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088ea74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    config_class = XLMRobertaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # Load model body\n",
    "        # we set add_​pool⁠ing_layer=False to ensure all hidden states are returned and not only the one associated with the [CLS] token.\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        # Set up token classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # Load and initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "                labels=None):\n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids)\n",
    "        # Apply classifier to encoder representation\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        # Return model output object\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits,\n",
    "                                     hidden_states=outputs.hidden_states,\n",
    "                                     attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "1a96e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2tag = {idx: tag for idx, tag in enumerate(class_names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(class_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b11fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n",
    "                                         num_labels=len(class_names),\n",
    "                                         id2label=index2tag,\n",
    "                                         label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e01ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "xlmr_model = (XLMRobertaForTokenClassification\n",
    "              .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "              .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "6bd3dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "sent_text = nltk.sent_tokenize(exText[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "20f744bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4349"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exText[0].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "aef56097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4349"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exLabel[0].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "68d1a65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4349\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in sent_text:\n",
    "    count+=len(i.split(' '))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "cd2318f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exTextSplit=exText[0].split(' ')\n",
    "theDF = pd.DataFrame([exText[0].split(' '),exLabel[0].split(',')],index=[\"Tokens\", \"Labels\"])\n",
    "theDF\n",
    "theDFT=theDF.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "62b8e9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Tokens', 'Labels'],\n",
       "    num_rows: 4349\n",
       "})"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfDS = Dataset.from_pandas(theDFT)\n",
    "\n",
    "dfDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "b1ff2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_testvalid = dfDS.train_test_split(test_size=0.2)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "ds = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "f06564c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Tokens', 'Labels'],\n",
       "        num_rows: 3479\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Tokens', 'Labels'],\n",
       "        num_rows: 435\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Tokens', 'Labels'],\n",
       "        num_rows: 435\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "877ff6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = xlmr_tokenizer(examples[\"Tokens\"], truncation=True,\n",
    "                                      is_split_into_words=False)\n",
    "    labels = []\n",
    "    for label in enumerate(examples[\"Labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids()\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels,batched=True,\n",
    "                        remove_columns=['Tokens','Labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edac772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "03898048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3479/3479 [00:00<00:00, 38217.02 examples/s]\n",
      "Map: 100%|██████████| 435/435 [00:00<00:00, 20633.79 examples/s]\n",
      "Map: 100%|██████████| 435/435 [00:00<00:00, 63765.50 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3479\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 435\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 435\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodedDS = encode_panx_dataset(ds)\n",
    "encodedDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "3dc586ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encodedDS['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d56e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "    # Function to take a set of predictions and align them with a list of labels to use in the classification report\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            # Ignore label IDs = -100\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    return preds_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "2f5baf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "num_epochs = 15\n",
    "batch_size = 24\n",
    "logging_steps = len(ds[\"train\"]) // batch_size\n",
    "model_name = f\"{xlmr_model_name}-finetuned-panx-en\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  log_level=\"error\",\n",
    "                                  num_train_epochs=num_epochs,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  eval_strategy=\"epoch\",\n",
    "                                  save_steps=1e6,\n",
    "                                  weight_decay=0.01,\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  push_to_hub=False,\n",
    "                                  report_to='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "9b5a2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions,\n",
    "                                       eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "2c997bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "532a437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return (XLMRobertaForTokenClassification\n",
    "            .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "            .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "d0d315e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3479\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 435\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 435\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodedDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd472909",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args,\n",
    "                  data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                  train_dataset=encodedDS[\"train\"],\n",
    "                  eval_dataset=encodedDS[\"validation\"],\n",
    "                  processing_class=xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "1e4718cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='2175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/2175 00:12 < 7:39:40, 0.08 it/s, Epoch 0.01/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[397], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Clarkson/Class Materials/2025_01_Spring/Natural Language Processing/.venv/lib/python3.9/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Clarkson/Class Materials/2025_01_Spring/Natural Language Processing/.venv/lib/python3.9/site-packages/transformers/trainer.py:2556\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2549\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2550\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2554\u001b[0m )\n\u001b[1;32m   2555\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2556\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2559\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2561\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2562\u001b[0m ):\n\u001b[1;32m   2563\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2564\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Documents/Clarkson/Class Materials/2025_01_Spring/Natural Language Processing/.venv/lib/python3.9/site-packages/transformers/trainer.py:3764\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3762\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3764\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/Documents/Clarkson/Class Materials/2025_01_Spring/Natural Language Processing/.venv/lib/python3.9/site-packages/accelerate/accelerator.py:2359\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2359\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Clarkson/Class Materials/2025_01_Spring/Natural Language Processing/.venv/lib/python3.9/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Clarkson/Class Materials/2025_01_Spring/Natural Language Processing/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Clarkson/Class Materials/2025_01_Spring/Natural Language Processing/.venv/lib/python3.9/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ee8b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13224a7a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc33cd5c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "772cfc17",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f81624a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
